{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4adf0a6",
   "metadata": {},
   "source": [
    "## Importação de bibliotecas e dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3981ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936525e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>mensagem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           mensagem\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table(\"./smsspamcollection/SMSSpamCollection\", sep=\"\\t\", header=None, names=[\"label\", \"mensagem\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f5fa7",
   "metadata": {},
   "source": [
    "## Data Pré-Processamento\n",
    "\n",
    "Agora que temos uma compreensão básica de como é nosso conjunto de dados, vamos converter nossos rótulos em variáveis binárias, 0 para representar 'ham' (ou seja, não spam) e 1 para representar 'spam' para facilitar o cálculo.\n",
    "\n",
    "Você pode estar se perguntando por que precisamos fazer esta etapa? A resposta para isso está em como o scikit-learn lida com as entradas. Scikit-learn lida apenas com valores numéricos e, portanto, se deixássemos nossos valores de rótulo como strings, scikit-learn faria a conversão internamente (mais especificamente, os rótulos de string serão convertidos em valores flutuantes desconhecidos).\n",
    "\n",
    "Nosso modelo ainda seria capaz de fazer previsões se deixássemos nossos rótulos como strings, mas poderíamos ter problemas mais tarde ao calcular as métricas de desempenho, por exemplo, ao calcular nossas pontuações de precisão e recall. Portanto, para evitar 'pegadinhas' inesperadas mais tarde, é uma boa prática fazer com que nossos valores categóricos sejam alimentados em nosso modelo como números inteiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19eec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>mensagem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           mensagem\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"] = data.label.map({\"ham\":0, \"spam\":1})\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a042d",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "O que temos aqui em nosso conjunto de dados é uma grande coleção de dados de texto (5.572 linhas de dados). A maioria dos algoritmos de ML depende de dados numéricos para serem inseridos neles como entrada, e as mensagens de e-mail/sms geralmente contêm muito texto.\n",
    "\n",
    "Aqui nós gostaríamos de apresentar o conceito Bag of Words (BoW) que é um termo usado para especificar os problemas que têm um 'bag of words' ou uma coleção de dados de texto que precisam ser trabalhados. A ideia básica do BoW é pegar um pedaço de texto e contar a frequência das palavras nesse texto. É importante notar que o conceito BoW trata cada palavra individualmente e a ordem em que as palavras ocorrem não importa.\n",
    "\n",
    "Usando um processo pelo qual passaremos agora, podemos converter uma coleção de documentos em uma matriz, com cada documento sendo uma linha e cada palavra(token) sendo a coluna, e os valores correspondentes (linha,coluna) sendo a frequência de ocorrência de cada palavra ou token nesse documento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15172f1",
   "metadata": {},
   "source": [
    "## Dividindo os dados em treino e teste\n",
    "\n",
    "Nosso primeiro passo é dividir nosso conjunto de dados em um conjunto de treinamento e teste para que possamos testar nosso modelo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad7c13dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas no conjunto total: 5572\n",
      "Número de linhas no conjunto de treinamento: 4179\n",
      "Número de linhas no conjunto de teste: 1393\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"mensagem\"], data[\"label\"])\n",
    "print('Número de linhas no conjunto total: {}'.format(data.shape[0]))\n",
    "print('Número de linhas no conjunto de treinamento: {}'.format(X_train.shape[0]))\n",
    "print('Número de linhas no conjunto de teste: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445620e5",
   "metadata": {},
   "source": [
    "Aplicaremos o Bag os Words e converteremos os nossos dados para o formato de matriz. Para fazer isso, usaremos CountVectorizer().Há duas etapas a serem consideradas aqui:\n",
    "\n",
    "- Em primeiro lugar, temos que ajustar nossos dados de treinamento (X_train) em CountVectorizer() e retornar a matriz.\n",
    "- Em segundo lugar, temos que transformar nossos dados de teste (X_test) para retornar a matriz.\n",
    "\n",
    "Observe que X_train são nossos dados de treinamento para a coluna 'mensagem' em nosso conjunto de dados e usaremos isso para treinar nosso modelo.\n",
    "X_test são nossos dados de teste para a coluna 'mensagem' e esses são os dados que usaremos (após a transformação em uma matriz) para fazer previsões. Em seguida, compararemos essas previsões com y_test em uma etapa posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6da3adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer()\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f90ac2ed",
   "metadata": {},
   "source": [
    "## Implementação Naive Bayes usando scikit-learn\n",
    "\n",
    "Utilizaremos o método sklearn.naive_bayes em nosso conjunto de dados.\n",
    "\n",
    "Especificamente, usaremos a implementação multinomial Naive Bayes. Este classificador específico é adequado para classificação com características discretas (como no nosso caso, contagem de palavras para classificação de texto). Ele recebe contagens de palavras inteiras como sua entrada. Por outro lado, o Gaussian Naive Bayes é mais adequado para dados contínuos, pois assume que os dados de entrada têm uma distribuição Gaussiana (normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb3d84c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dafd1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nb.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07feb757",
   "metadata": {},
   "source": [
    "Agora que as previsões foram feitas em nosso conjunto de teste, precisamos verificar a precisão de nossas previsões."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a80c16",
   "metadata": {},
   "source": [
    "## Avaliando nosso modelo\n",
    "\n",
    "Para problemas de classificação que são distorcidos em suas distribuições de classificação, como no nosso caso, por exemplo, se tivéssemos 100 mensagens de texto e apenas 2 fossem spam e as outras 98 não fossem, a precisão por si só não é uma métrica muito boa. Poderíamos classificar 90 mensagens como não spam (incluindo as 2 que eram spam, mas as classificamos como não spam, portanto, seriam falsos negativos) e 10 como spam (todos os 10 falsos positivos) e ainda obter uma pontuação de precisão razoavelmente boa. Para esses casos, precisão e recall são muito úteis. Essas duas métricas podem ser combinadas para obter a pontuação F1, que é a média ponderada das pontuações de precisão e recall. Essa pontuação pode variar de 0 a 1, sendo 1 a melhor pontuação possível na F1.\n",
    "\n",
    "Usaremos todas as 4 métricas para garantir que nosso modelo funcione bem. Para todas as 4 métricas cujos valores podem variar de 0 a 1, ter uma pontuação o mais próxima possível de 1 é um bom indicador do desempenho do nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34347a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia:  0.9856424982053122\n",
      "Precisão:  0.9479768786127167\n",
      "Recall:  0.9371428571428572\n",
      "F1 score:  0.942528735632184\n"
     ]
    }
   ],
   "source": [
    "print('Acurácia: ', format(accuracy_score(y_test, pred)))\n",
    "print('Precisão: ', format(precision_score(y_test, pred)))\n",
    "print('Recall: ', format(recall_score(y_test, pred)))\n",
    "print('F1 score: ', format(f1_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c586c671",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Uma das principais vantagens que o Naive Bayes tem sobre outros algoritmos de classificação é sua capacidade de lidar com um número extremamente grande de recursos. No nosso caso, cada palavra é tratada como um recurso e existem milhares de palavras diferentes. Além disso, ele funciona bem mesmo com a presença de recursos irrelevantes e é relativamente inalterado por eles. A outra grande vantagem que tem é a sua relativa simplicidade. O Naive Bayes funciona bem imediatamente e o ajuste de seus parâmetros raramente é necessário, exceto geralmente nos casos em que a distribuição dos dados é conhecida. Raramente supera os dados. Outra vantagem importante é que os tempos de treinamento e previsão do modelo são muito rápidos para a quantidade de dados que ele pode manipular. Em suma, Naive Bayes é realmente uma jóia de algoritmo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6412f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
